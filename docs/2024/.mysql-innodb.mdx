---
title: InnoDB
date: 2024-08-17
---


## yaml


```yaml
- topic: InnoDB
  qs:
    - q: "***InnoDB, feats? (checkpoint, buffer-pool, read-ahead, insert-buffer, double-write, adaptive-hash-index) 这些都是啥?***"
      u: https://lotabout.me/2020/Book-Notes-InnoDB-Engine-2nd-edition/
      x: |
        - 事务相关：支持 ACID 事务、MVCC、InnoDB 锁机制、redolog
        - 性能相关：插入缓冲、double write、BP(buffer pool)、AHI(Adaptive Hash Index)自适应哈希索引

        但是实际上事务相关特性，本质上也还是为了优化性能，比如说之所以用 MVCC 就是为了能够并发事务，而 InnoDB 锁机制和 redolog 本质上来说都是为了保证 MVCC 正常运行。

        另外，也有一些其他特性，比如外键、行级锁定、共享表空间和独立表空间等等，这些都比较简单，就不太重要了。

        ---

        ```markdown
        - `插入缓冲 (insert buffer)`，*加速插入操作*，插入缓冲用于非聚簇索引的插入和更新操作，先判断插入的非聚簇索引是否在缓存池中，如果在则直接插入，否则插入到`插入缓存对象`中。再以一定的频率进行插入缓冲和辅助索引叶子节点的 merge 操作，将多次插入合并到一个操作中，提高对非聚簇索引的插入性能
        - `二次写 (double write)`由两部分组成，一部分是内存中的`double write buffer`，大小为 2MB，另一部分是物理磁盘上共享表空间连续的 128 个页，大小也为 2MB。在对缓冲池的脏页刷新时，并不直接写磁盘，而是通过 memcpy 函数将脏页先复制到内存中的该区域，之后通过`double write buffer`再分两次，每次 1MB 顺序地写入共享空间的物理磁盘上，然后马上调用 fsync 函数，同步磁盘，避免 OS 缓冲写带来的问题
        - `自适应哈希索引 (adaptive hash index)`*自动在内存中创建 hash 索引来加速读操作*，innoDB 会根据访问的频率和模式，为热点页建立哈希索引，来提高查询效率。索引通过缓存池的 B+ 树页构造而来，因此建立速度很快，innoDB 存储引擎会监控对表上各个索引页的查询，如果观察到建立哈希索引可以带来速度上的提升，则建立哈希索引，所以叫做`自适应哈希索引`
        - `缓存池`为了提高数据库的性能，引入缓存池的概念，通过参数可以设置缓存池的大小和实例个数，缓存池可以存储一下内容：索引页、数据页、undo 页、插入缓冲、自适应哈希索引、innoDB 存储的锁信息和数据字典信息 (data dict)
        - *使用可预测读作为磁盘数据读取方式*

        ### 缓冲池

        - 什么是缓冲池 buffer-pool？
        - *避免每次读操作都进行磁盘 IO*，具体来说，缓冲池缓存了大量数据页，让 CPU 读取和写入数据时，直接和缓冲区交互，不需要操作磁盘，从而避免磁盘拖慢数据库性能的问题（*注意缓冲池是 innoDB 引擎的特性，而不是 mysql 的*）
        - 缓冲池存哪些数据？
        - *缓存表数据与索引数据*，把磁盘上的数据加载到缓冲池，避免每次访问都进行磁盘 IO，起到加速访问的作用。
        - 缓冲池的工作机制？

        ### 预读

        - 什么是预读？磁盘按页读取，如果要读取的数据就在页中，就能节省后面的磁盘 io，提高效率。数据访问遵循`集中读写`的原则，使用一些数据，大概率会使用附近的数据，这就是`局部性原理`
        - 什么是预读失败？
        - 如何对预读失败进行优化？
        - 什么是缓冲池污染？
        - 怎么解决缓冲池污染的问题？
        ```

    - q: "*InnoDB, arch? InnoDB数据页的数据结构? How does it works?*"
      x: |
        各个数据页可以组成一个 双向链表 ，而每个数据页中的记录会按照主键值从小到大的顺序组成一个 单向链表，每个数据页都会为存储在它里边儿的记录生成一个页目录 ，在通过主键查找某条记录的时候可以在 页目录 中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录(如果你对这段话有一丁点儿疑惑，那么接下来的部分不适合你，返回去看一下数据页结构吧)。

        通过双向链表相关联

    - q: InnoDB 脏页刷新
      p: https://docs.hxha.xyz/img/db/mysql/innodb-page-flushing.svg


    - q: innodb索引树的高度由什么决定?
      u: https://www.bilibili.com/video/BV1jb421e7iS/
      x: 单条数据的大小会影响索引树的高度。bptree是由16k的page组成的，非叶子node只存index，叶子node存储index和数据本身。


    - q: InnoDB 为什么要使用 bptree？
    - q: 为啥mysql的InnoDB使用bptree，而不是btree或者skiptable？

    - q: redolog 是啥？为什么需要记录 redolog？
      x: |
        *undolog是逻辑日志，redolog是物理日志。但是redolog是由undolog产生的*

        redolog其实就是WAL，通过先记录日志再写入数据的方式，确保了事务的持久性，并通过Checkpoint技术有效管理了日志空间和恢复时间
    - q: 需要什么样的 redolog？
      x: 为了获得更好的读写性能，innoDB 将数据缓存到内存 (innoDB Buffer Pool)，对磁盘数据的修改也会落后于内存，如果进程崩溃就会导致内存数据丢失，所以 innoDB 就维护了 redolog，内存数据丢失后，innoDB 会在重启时，通过重放 REDO，恢复数据
    - q: redolog 中记录了什么内容？
    - q: redolog 是怎么组织的？
    - q: 如何高效地写 redolog？
    - q: 如何安全地清除 redolog？

    - q: "***checkpoint是啥? 只跟redolog 相关吗? 跟其他日志有关吗?***"
      x: |
        其实就是刷盘操作（当然还有其他刷盘操作，具体的触发条件、执行方式和优化策略不同，比如同步写入、异步写、定时写等等）

        用来把内存中的脏页（尚未写入磁盘的修改数据页）写入磁盘，并更新相关的日志信息，这个操作是为了保证数据的一致性，以防止系统崩溃时数据丢失。

        - sharp checkpoint: mysql关闭时，会触发把所有的脏页都刷入到磁盘上
        - fuzzy checkpoint: mysql运行时，部分刷入磁盘

```



```yaml
- topic: InnoDB 缓冲池(buffer-pool)
  qs:
    - q: 避免每次读操作都进行磁盘 IO，具体来说，缓冲池缓存了大量数据页，让 CPU 读取和写入数据时，直接和缓冲区交互，不需要操作磁盘，从而避免磁盘拖慢数据库性能的问题（*注意缓冲池是 innoDB 引擎的特性，而不是 mysql 的*）
    - q: 缓冲池存哪些数据？
      x: 缓存表数据与索引数据，把磁盘上的数据加载到缓冲池，避免每次访问都进行磁盘 IO，起到加速访问的作用。
    - q: 缓冲池的工作机制？

- topic: InnoDB 预读
  qs:
    - q: 什么是预读？
      x: 磁盘按页读取，如果要读取的数据就在页中，就能节省后面的磁盘 io，提高效率。数据访问遵循`集中读写`的原则，使用一些数据，大概率会使用附近的数据，这就是`局部性原理`
    - q: 什么是预读失败？
    - q: 如何对预读失败进行优化？
    - q: 什么是缓冲池污染？
    - q: 怎么解决缓冲池污染的问题？
```





InnoDB的各种基础概念非常多，如果不看源码真的被各种乱七八糟的概念搞的头疼，但是源码又看不懂，怎么办？




在开始阅读blog之前，需要想清楚一个问题：

> [!TIP]
> Innodb相较于其他存储引擎的核心功能是什么？为了实现这个功能，InnoDB相较于其他存储引擎实现了哪些机制？

毫无疑问，***InnoDB的最核心功能就是tx***

那么把这个问题再延伸

> [!TIP]
> 那pgsql也支持事务支持，为啥pgsql基于MVTO而不是MV2PL实现MVCC，pgsql使用heap table（类似mysql中memory引擎）而不是HOT呢？


***这个问题就是解开InnoDB设计的钥匙***





厘清InnoDB基础概念


InnoDB的相关机制太TM多了，

说到



innodb结构



innodb的核心是tx,

insert buffer, double write, adaptive hash index, buffer pool

checkpoint, page-flush

buffer-pool, pre-reading,

这些概念都太乱了，更能给予这些概念延伸出很多问题







## InnoDB结构


```markdown
如果我们把存储引擎比做图书馆的所有书架的话，表、页、行分别类比成什么呢？表可以类比成图书馆的某个区域，每个区域存放着特定类型的书籍。页可以看作是该区域的某个书架，每个书架上放着一定数量的书籍。行可以类比成某一排书，每一排上放着一本书。

进一步的，我们可以用这些继续类比myisam和innodb，myisam就是每个分类（比如工业技术、交通运输、历史地理等等，具体查看《中国图书馆分类法》）都会有一个单独的目录，并且不允许很多人同时查找（不支持并发查找，也就是不支持事务），进行查找时直接锁表，相应的，我们可以直接通过这个单独的MYI索引文件查找对应的图书，非常快。

InnoDB就灵活的多了，他是以书架为单位进行查找和存储的，所以他的目录（索引）是和某排图书在一起的（相当于每排书一个索引（主键索引），索引id都贴在书架该层的最前面，这时就存在两种情况，一种就是直接查目录（索引）就可以获得数据，不需要再查数据表，这个就是“索引覆盖”（或者说Index Dive，都是类似意思）。如果目录中查不到我们需要的数据，就需要“回表”了，那么我们根据索引id来看需要的书是不是在这排（再多说一句，这也是为什么主键id最好单调递增的原因））。他是支持多人同时查找的。并且在读写操作时只锁定某排书（也就是行级锁定），粒度更细，这样就更能频繁地执行读操作和写操作。

需要注意的是mysql还支持同时使用多种存储引擎，也就是说在图书馆里如果有一些冷门分类，很少有新书入库的，那就用myisam这种类型的管理方式，如果非常热门的分类，每天都有很多用户来买书，每天书店也需要频繁补货的，就应该用innodb。当然，如果我们把mysql看作是图书馆的话，不同图书馆的特性（规矩、规章制度）也不同，比如说oracle或者sql server这种的就比较死板，图书馆开店营业之前就定好了所有书架的管理模式（也就是存储引擎），之后再也不能修改，更不存在说不同分类使用不同的管理模式一说了。

另外，还需要注意一个问题，编织目录（索引）的具体方法和该“图书分类”（存储引擎）的映射关系。众所周知，对于不同图书分类也应该使用不同的方式来编写目录。比如说R树适用于myisam（也有B+Tree），hash适用于memory，B+Tree则适用于InnoDB。
```

正如我之前 InnoDB和图书馆书架 的类比




mysql-innodb-arch.png

```markdown
InnoDB引擎，主要包括两部分，一部分是InnoDB内存结构，另一部分就是InnoDB磁盘结构。

在内存结构里面，主要就是各种buffer缓冲区，这些缓冲区对于提升数据库性能上至关重要。

Buffer Pool，缓冲池。缓冲池允许直接从内存中处理经常使用的数据，从而加快了处理速度。在专用服务器上，通常将多达 80％的物理内存分配给缓冲池。
Change Buffer，更改缓冲区。这块缓冲区，是存放在Buffer Pool内部。更改缓冲区是一种特殊的数据结构，当这些页面不在Buffer Pool中时，该缓存可缓存对二级索引页的更改。可能由INSERT、UPDATE，或 DELETE操作(DML)导致的缓冲更改将在以后通过其他读取操作将页面加载到缓冲池中时合并。也就是说，其对应的数据不需要从磁盘加载到Buffer中，而是将变更存在在Change Buffer内，等待合适时机再merge 回去。
Log Buffer，日志缓冲区。主要用来存放redo log 数据。
在磁盘结构里面，就是各种类型的文件。所有的数据，最终都需要落到各种磁盘文件上来保证数据的持久性。核心的文件主要有：

表和索引文件。
double write buffer文件。
undo log 文件。
redo log 文件。
表空间、表、索引这些文件，是真正数据文件。double write buffer 是由于linux页大小4K，MySQL页大小16K，直接将MySQL页写到磁盘，可能导致部分成功、部分失败导致后期数据恢复存在问题，因为增加了double write buffer做数据恢复的时候使用。undo log事务回滚和MVCC时需要找到历史快照数据。redo log 则是保证数据的持久和原子性而存在的。
```

正如上图






## 脏页刷新







## Lock

```yaml
# [秒懂InnoDB的锁 - 菜刚RyuGou的博客](https://i6448038.github.io/2019/02/23/mysql-lock/)
- topic: InnoDB Lock
  qs:
    - q: "***FTWRL (Forced Transactional Write-Ahead Logging)***"
      x: |
        ***用来阻塞所有写操作，来保证备份时的数据一致性***

        FTWRL用来确保即使mysql crash, 已经提交的事务也不会丢失，因为事务日志已经被安全地写入到磁盘。这对于需要高数据一致性和可靠性的应用场景非常重要。


    # [innodb源码分析之基础数据结构 · innodb源码分析 · 看云](https://www.kancloud.cn/digest/innodb-zerok/195085)
    # [MySQL 之 InnoDB 锁系统源码分析 - 掘金](https://juejin.cn/post/7075853223625490468)
    # [秒懂InnoDB的锁 - 菜刚RyuGou的博客](https://i6448038.github.io/2019/02/23/mysql-lock/)
    # [MySQL 全局锁、表级锁、行级锁，你搞清楚了吗？](https://mp.weixin.qq.com/s?__biz=MzUzNTY5MzU2MA==&mid=2247496937&idx=1&sn=398da5cc033c93f5a73e75c585fc9dde)
    - q: "***聊聊innodb锁机制？***"
      u: https://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&mid=2651967369&idx=1&sn=d639abf6772a72c25cc2537749258163
      x: |
        - 共享锁 (S锁, shared lock)
        - 排他锁 (X锁, exclusive lock)
        - 意向锁  (I锁, intention lock)
        - 意向共享锁 (IS锁, 表示事务准备给数据行加入共享锁，也就是说一个数据行加共享锁前必须先取得该表的IS锁)
        - 意向排他锁 (IX锁, 类似上面，表示事务准备给数据行加入排他锁，说明事务在一个数据行加排他锁前必须先取得该表的IX锁)
        - 插入意向锁  (insert intention lock)
        - 自增锁  auto-inc lock

        行锁

        - 记录锁  (record lock)
        - 间隙锁  (gap lock)
        - 临键锁  (next-key lock)

        我说下我的理解哈，shared lock和exclusive lock就是mutex和rwmutex,所有语言里都有类似的锁机制，不多说。intention lock其实就是IsLocked，用来判断是否加锁，进而决定下一步操作是sleep还是spin什么的，也很常用。下面的gaplock，next-key lock 都是MVCC相关的锁，可以理解。自增锁auto-inc lock是用来保证自增字段唯一性的，防止并发插入时，某个自增字段的数据重复。我不太理解record lock和insert intention lock有啥用。mutex(exclusive lock)不是已经能够在写操作时的行级锁定了吗?为什么还需要record lock呢?还有不是已经有intention lock了吗?为什么还需要insert intention lock?


    - q: 多粒度锁机制
      x: 行锁分为共享锁和排他锁，但是 InnoDB 还有两种内部使用的意向锁，这两种意向锁都是表锁。表锁、页锁、行锁、字段锁，当然是锁粒度更小的，开销更小、加锁更快，这个是毫无疑问的。

    - q: 怎么理解插入意向锁? 到底是I锁还是gap锁?
    - q: "***innoDB 间隙锁是什么？innoDB 间隙锁有哪些缺点？***"
      x: |
        InnoDB 的锁定是通过在指向数据记录的第一个索引键之前和最后一个索引键之后的空域空间标记锁定信息实现的。这种锁定方式被称为 "NEXT-KEY locking"（间隙锁）

        锁定一个范围之后，即使某些不存在的键值也会被无辜锁定，造成锁定的时候无法插入键值锁定内的任何数据

    - q: MySql Lock wait timeout exceeded该如何处理?
      x: |
        Lock wait timeout exceeded; try restarting transaction

        死锁导致事务超时

        update语句锁全表导致的等待超时问题
```


## MVCC


```yaml
- topic: Transaction/MVCC
  qs:
    - q: 什么是 CC？有哪些单版本的 CC 方法？基于这些单版本 CC 方法，MVCC 有哪些不同的实现?
      x: CC 由数据库的调度器负责，事务本身感知不到可能导致数据一致性的冲突事务，调度器会 delay 或者 abort，如果 delay 就延迟到合法时机，如果 abort 就直接回滚。本质上是一个取舍问题，乐观锁不维护锁，吞吐很高，但是相应回滚也会比较多，而回滚比延迟的成本要高很多，所以在冲突较少和 validation 开销小的情况下，使用 OCC。LBCC 的方案则相反。

    - q: "***三种实现 MVCC 的方法? MVCC data structure, How to implement MVCC?***"
      u: https://catkang.github.io/2018/09/19/concurrency-control.html
      x: |
        其实就是三种，乐观锁、悲观锁和ts。也就是 MV-2PL(悲观锁)和MV-OCC(乐观锁，读写时不做验证，延迟到提交时验证)，以及MV-TO(ts-based)

        - *CC 由数据库的调度器负责，事务本身感知不到可能导致数据一致性的冲突事务，调度器会 delay 或者 abort*，如果 delay 就延迟到合法时机，如果 abort 就直接回滚
        - 本质上是一个取舍问题，*乐观锁不维护锁，吞吐很高，但是相应回滚也会比较多，而回滚比延迟的成本要高很多，所以在冲突较少和 validation 开销小的情况下，使用 OCC。LBCC 的方案则相反*

        CC 的方法

        - Lock，2PL，事务开始时，delay，
        - timestamp，操作进行时，delay/abort，
        - validation，最终提交前，abort，

        *MVCC 可以基于乐观锁和悲观锁实现 OCC 和 PCC*，也就是

        - `MV-2PL`，2PL，LBCC，lock，基于锁实现 MVCC，比如 MySQL
        - `MV-TO`，timestamp，基于时间排序实现 MVCC，比如 PgSQL
        - `MV-OCC`，OCC，validation，读写时不做验证，延迟到提交时验证

        ---

        ```markdown
        这里从两个维度，对常见的并发控制机制进行分类：

        1. 乐观程度
        不同的实现机制，基于不同的对发生冲突概率的假设，悲观方式认为只要两个事务访问相同的数据库对象，就一定会发生冲突，因而应该尽早阻止；而乐观的方式认为，冲突发生的概率不大，因此会延后处理冲突的时机。如上图横坐标所示，乐观程度从左向右增高：

        1、基于Lock：最悲观的实现，需要在操作开始前，甚至是事务开始前，对要访问的数据库对象加锁，对冲突操作Delay；
        2、基于Timestamp：乐观的实现，每个事务在开始时获得全局递增的时间戳，期望按照开始时的时间戳依次执行，在操作数据库对象时检查冲突并选择Delay或者Abort；
        3、基于Validation：更乐观的实现，仅在Commit前进行Validate，对冲突的事务Abort

        可以看出，不同乐观程度的机制本质的区别在于，检查或预判冲突的时机，Lock在事务开始时，Timestamp在操作进行时，而Validation在最终Commit前。相对于悲观的方式，乐观机制可以获得更高的并发度，而一旦冲突发生，Abort事务也会比Delay带来更大的开销。
        ```

        ```markdown
        对应上述每种乐观程度，都可以有多版本的实现方式，多版本的优势在于，可以让读写事务与只读事务互不干扰，因而获得更好的并行度，也正是由于这一点成为几乎所有主流数据库的选择。为了实现多版本的并发控制，需要给每个事务在开始时分配一个唯一标识TID，并对数据库对象增加以下信息：

        txd-id，创建该版本的事务TID
        begin-ts及end-ts分别记录该版本创建和过期时的事务TID
        pointer: 指向该对象其他版本的链表

        其基本的实现思路是，每次对数据库对象的写操作都生成一个新的版本，用自己的TID标记新版本begin-ts及上一个版本的end-ts，并将自己加入链表。读操作对比自己的TID与数据版本的begin-ts，end-ts，找到其可见最新的版本进行访问。根据乐观程度多版本的机制也分为三类：
        ```

        以 乐观程度 和 是否MV 为横纵坐标，可以看到pgsql就使用MVTO，而mysql（InnoDB）和oracle则使用MVRC(MM read consistency)

        ```markdown
        PostgreSQL uses multi-version timestamp ordering (MVTO) while InnoDB and Oracle use multi-version read consistency (MVRC). The main difference is that PostgreSQL is with-REDO/no-UNDO because it stores every row version in the main table, while Oracle/InnoDB implements with-REDO/with-UNDO where they reconstruct a block and/or row image from the log to provide read consistency.
        ```

        *MVCC具体起到什么作用？我知道MVCC是用来实现再db中并发读写操作时保证数据一致性，那从CAP的角度出发，是为了保证Consistency吗？*

        与其说MVCC是为了保证C，不如说是为了保证在C和A之间的平衡（想想之所以说InnoDB是50%C +50%A，就知道了）


    - q: "***MySQL 2PL***"
      x: 2PL就是通过组合使用innodb的S锁和X锁来在保证事务隔离的情况下，提高并发性能



    - q: "***InnoDB是怎么保证事务ACID的？***"
      x: |
        - `Consistency`，*一致性是最基本属性，其他三种都是为了实现一致性而存在的*
        - `Isolation`，用事务的`隔离级别`保证事务的隔离性，为了保证并发场景下的一致性，引入隔离性，不同事务之间互不影响
        - `Atomic`，用 undolog 保证事务执行失败后，直接回滚
        - `Durability`，用 redolog 保证事务提交后，对数据的修改是永久性的，即使系统故障也不会丢失

        事务的 ACID 特性分别是什么？

        - `Atomic`，事务里的所有操作要么全部完成，要么全部失败
        - `Consistency`，事务执行一定要保证系统的一致性
        - `Isolation`，事务与事务之间不会互相影响，一个事务的中间状态不会被其他事务感知
        - `Durability`，事务一旦完成，所有写操作都要持久化

    - q: 数据库事务隔离发展历史
      u: https://catkang.github.io/2018/08/31/isolation-level.html
      x: |
        ```markdown
        ANSI SQL标准(1992)：基于异象
        A Critique of ANSI(1995)：基于锁
        A Generalized Theory(1999)：基于序列化图

        Cursor Stability
        Snapshot Ioslation
        ```

        ANSI SQL92 用三种并发异常（Anomaly），划分出四种不同的隔离级别，将这种利弊权衡进行了（糟糕的）标准化



    - q: MySQL对MVCC的实现? MVCC = 版本链+ReadView(undolog)
      x: |
        通过版本链的 trx_id 和 ReadView 的高低水位比较后，决定使用哪个版本。ReadView 就是快照，用来做可见性判断。

        - ReadView 的结构 (m_ids, min_trx_id 低水位, max_trx_id 高水位, creator_trx_id)
        - undolog 版本链的结构 (trx_id 事务 id, roll_pointer 回滚指针)


    - q: "***事务的四种隔离级别是什么？RU、RC、RR、SR 分别是什么?***"
      u: https://lotabout.me/2020/QQA-Isolation-Level-of-Database/
      x: |
        隔离就是类似linux kernel的可见性，通过可见性来解决事务在并发请求（包括读写操作）时的数据一致性问题。比如什么RU、RC、RR、SR，随着隔离级别的加强，分别解决了脏写、脏读、不可重复读和幻读的问题。隔离级别越来越高，并发性就越差。当多个事务并发时，MySQL 利用加锁和阻塞来保证事务之间不同等级的隔离性。

        RU、RC、RR、SR 分别是什么？

        - `RU`读操作不加锁，可能会`脏读`(解决了`脏写`问题)
        - `RC`只对记录加记录锁，而不会在记录之间加间隙锁，所以允许新的记录插入到被锁定记录的附近。所以在多次读操作时，会发生`不可重复读`(解决了`脏读`问题)
        - `RR`多次读取同一范围的数据会返回第一次查询的快照，不会返回不同的数据行，但是可能会发生`幻读`(解决了`不可重复读`问题)
        - `SR`InnoDB 隐式地将全部查询语句加上共享锁，解决了`幻读`问题，但是性能很差

        ---

        RC隔离

        Read Committed（读取已提交）

        - 只读取提交的数据，并等待其他事务释放排他锁。
        - 这是大多数数据库系统的默认隔离级别（但不是 MySQL 默认的）。
        - 它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。
        - 这种隔离级别也支持所谓的不可重复读，因为同一事务的其他实例在该实例处理期间，可能会有新的 commit，所以同一个 select 可能返回不同结果

        RU隔离

        Read Uncommitted（读取未提交内容）

        - 在读数据时，不会检查或者使用任何锁。
        - 在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。*RU 很少被使用，因为它的性能比其他隔离级别好不了太多，并且会导致脏读*

        RR隔离

        Repeatable Read（可重复读）

        事务 A 读取一条数据后，事务 B 对该数据修改并提交，事务 A 再读取该数据，读到的还是原来的内容

        - 像 RC 那样读数据，但会保持共有锁，直到事务结束
        - 这是 MySQL 的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。
        - 不过理论上，这会导致另一个棘手的问题：幻读，简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影”行。
        - InnoDB 和 Falcon 存储引擎通过 MVCC 机制解决了该问题。(*其实多版本只是解决不可重复读问题，而加上间隙锁 (也就是它这里所谓的并发控制) 才解决了幻读问题*)

        SR隔离

        Serializable（可串行化）

        - 工作方式类似于可重复读。但它不仅会锁定受影响的数据，还会锁定这个范围。这就阻止了新数据插入查询所涉及的范围。
        - 这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。

    - q: "***RR 和 RC 的区别？RC有幻读，为了解决幻读所以用的RR，那 RC 和 RR 的 MVCC 机制有啥不同呢？各自会解决和导致什么问题?***"
      x: |
        *RC 和 RR 的区别在于 ReadView 快照生成时机不同，导致可见性不同*

        - RC 在每次读取数据前都生成一个 ReadView
        - RR 在第一次读取数据时生成一个 ReadView

        ---

        - RR 和 RC 在锁方面的区别？
        - RR 和 RC 在复制方面的区别？
        - RR 和 RC 在一致性读方面的区别？RC 支持半一致性读，RR 不支持

        幻读解决办法是rr 隔离级别加的间隙锁 +一致性非锁定读(mvcc)解决的。


    - q: 为什么 MySQL 把 RR 作为默认的隔离级别呢? MySQL 是怎么实现 RR 的?
      x: Lost Update 和 Constraint Violation 都是 Fuzzy Read 的变种


    - q: RR 和 RC 在锁、复制、一致性读方面的区别?
      x: "***RC 和 RR 的区别在于 ReadView 快照生成时机不同，导致可见性不同***，RC 在每次读取数据前都生成一个 ReadView，而RR 在第一次读取数据时生成一个 ReadView"


    - q: "*MySQL（中 InnoDB）的事务是怎么实现的？怎么实现 RR 的?*"
    - q: "***不同隔离级别，可能导致哪些问题?***"
    - q: 并发事务之间的相互影响?
    - q: 脏读、不可重复读、幻读分别是什么?
      x: |
        - `脏读`，t1 在修改之前提交，t2 读取，t1 回滚，t2 读取了从未提交的数据，*读未提交时，读事务直接读取主记录，无论更新事务是否完成*
        - `不可重复读`t1 读取，t2 修改该数据并提交，t1 重新读取，数据已被修改，数据不同
        - `幻读`数据不同*每次都能读到最新数据*

        ---

        *不同的隔离级别有不同的现象，并有不同的锁定/并发机制，隔离级别越高，数据库的并发性就越差*，对于不同的事务，采用不同的隔离级别分别有不同的结果

        - `脏读`一个事务可以读取另一个尚未提交事务的修改数据 (一个事务内修改了数据，另一个事务读取并使用了这个数据)
        - `不可重复读`在同一个事务中，同一个查询在 T1 时间读取某一行，在 T2 时间重新读取这一行时候，这一行的数据已经发生修改，可能被更新了，也可能被删除了 (一个事务内连续读了两次数据，中间另一个事务修改了这个数据，导致第一个事务前后两次读的数据不一致)
        - `幻读`在同一事务中，同一查询多次进行时候，由于其他 insert 的事务提交，导致每次返回不同的结果集 (一个事务内修改了涉及全表的数据，另一个事务往这个表里面插入了新的数据，第一个事务出现幻读)
        - `更新丢失`一个事务内变更了数据，另一个事务修改了这个数据，最后前一个事务 commit 导致另一个事务的变更丢失



    - q: MySQL 遇到脏读怎么办?

    - q: "***MVCC 机制的原理和实现：MVCC 是什么?ReadView 是什么？高水位、低水位?***"
      x: |
        InnoDB 使用 MVCC 来支持高并发，并且实现了四个标准的隔离级别，通过间隙锁 next-key locking 策略防止幻读的出现

        - MVCC 就是*读请求直接访问对应版本的数据，从而避免读写事务和只读事务互相阻塞*，同一个数据有多个版本，*最大的好处是读写不冲突，只有同时写操作冲突，可以很大程度上提升性能*
        - MVCC 的目的就是，实现数据库的隔离级别

        ***MVCC=版本链+ReadView（就是快照，用来做可见性判断）***

        通过版本链的 trx_id 和 ReadView 的高低水位比较后，决定使用哪个版本

        - ReadView 的结构
          - m_ids
          - min_trx_id 低水位
          - max_trx_id 高水位
          - creator_trx_id
        - undolog 版本链的结构
          - trx_id，事务 id
          - roll_pointer 回滚指针



    - q: "*RR 是怎么实现的？RR 怎么用间隙锁解决幻读?* (常见问题)"

    - q: MySQL 的 RR 无法避免 PMP / G-Single 异常，Hermitage 将其实际等级定为 单调原子视图/MAV
      u: https://zhuanlan.zhihu.com/p/675251957

    - q: MySQL 事务中的加锁和解锁时机？
      x: |
        - 对记录进行更新操作，或者`select for update`(X 锁)、`lock in share mode`(S 锁) 时，会对记录进行加锁，锁的种类很多，不赘述
        - 在一个事务中，只有在`commit`或者`rollback`时，才会解锁


```





## ???



```markdown
这句话可以从以下几个关键点来理解InnoDB的redo log以及它如何实现WAL（Write-Ahead Logging）策略：

1. **WAL策略**：WAL是一种常见的数据库日志技术，核心思想是在数据实际写入磁盘之前，先将对数据的修改操作记录到日志中。这样做可以提高性能，因为写日志通常比直接写磁盘要快，并且可以减少对磁盘的I/O操作。

2. **事务的持久性**：事务的持久性是ACID原则中的"D"，指的是一旦事务提交，它对数据库的改变就是永久性的，即使系统发生故障也不会丢失。InnoDB通过redo log来实现这一点：当事务提交时，相关的修改操作已经记录在redo log中，如果系统发生故障，InnoDB可以重放redo log中的操作，从而恢复未持久化到磁盘的数据。

3. **先记录日志再写入数据**：这是WAL策略的具体实现。在InnoDB中，当事务需要修改数据时，首先会在redo log中记录这次修改的详细信息，然后再将修改后的页（数据）写入内存中的缓冲池。这样做可以确保即使在数据页写入磁盘之前系统崩溃，通过redo log也能够恢复数据。

4. **Checkpoint技术**：Checkpoint是InnoDB中用于管理redo log空间和优化恢复时间的一种机制。它在redo log中标记了一个点，表示在这个点之前的所有事务都已经将数据持久化到磁盘。这样，当系统需要恢复时，只需要重放Checkpoint之后的部分日志，而不是整个日志文件，从而大大减少了恢复所需的时间。

5. **有效管理日志空间**：由于redo log是循环使用的，Checkpoint还有助于释放不再需要的日志空间。一旦数据页被刷新到磁盘并且对应的事务已经提交，从Checkpoint点之前的日志就可以被清除，为新的事务日志腾出空间。

综上所述，InnoDB的redo log通过WAL策略确保了事务的持久性，并通过Checkpoint技术有效管理了日志空间和恢复时间，从而提供了高性能和数据安全性。

```






InnoDB的redolog就是WAL嘛，必然会产生脏页



为啥会出现脏页? page flush的时机? 刷新策略?


能否给我概述一下整个page flush的过程？整个flush过程中涉及到哪些innodb的特性？



flush page 就是把内存中修改过的页（脏页）写回到磁盘上，以确保数据的持久性和一致性。

之所以产生脏页就是因为InnoDB用WAL来防止宕机数据丢失，也就是事务提交时，先写redolog，再修改内存数据页，这样就产生了脏页。

那为啥要刷新呢？因为不可能把所有 buffer pool 所以需要把buffer pool里的脏页都刷新到磁盘里

InnoDB通过LSN（每条log的结束点，用字节偏移量来表示。每个page有LSN，redo log也有LSN，Checkpoint也有LSN）来标记版本

Checkpoint机制每次刷新多少页，从哪里取脏页，什么时间触发刷新？




```markdown
当然，以下是InnoDB存储引擎中脏页刷新（Page Flush）过程的概述，涵盖了所有关键点：

1. **事务修改页**：当事务对表中的数据进行修改时，这些修改首先在内存中的页（buffer pool中的页）上进行。一旦页被修改，它就变成一个脏页。

2. **重做日志记录**：在事务提交之前，InnoDB会将修改记录到重做日志（redo log）中。这是为了确保在发生故障时可以使用这些日志来恢复数据。

3. **事务提交**：事务提交后，修改的页仍然是脏页，因为它们还没有被写回到磁盘上。

4. **脏页刷新触发**：InnoDB有几种机制来触发脏页的刷新：
   - **后台刷新**：后台线程（Page Cleaner）定期刷新脏页。
   - **Checkpoint触发**：当重做日志的头部（head）和尾部（tail）之间的距离（checkpoint age）达到一定阈值时，会触发刷新。
   - **缓冲池压力**：当缓冲池中脏页的比例过高或需要为新的读操作腾出空间时，会触发脏页刷新。

5. **刷新邻接页**：如果启用了刷新邻接页（Flush Neighbor Page）功能，InnoDB在刷新一个脏页时，会检查同一区（extent）内的所有页，如果这些页也是脏的，它们也会被一起刷新。

6. **自适应刷新**：InnoDB的自适应刷新算法根据当前的写入负载和磁盘性能动态调整刷新速率，以避免刷新风暴（flush storm）。

7. **刷新操作**：脏页被写回到磁盘上，这一过程可能涉及异步IO操作，以提高效率。

8. **刷新完成**：一旦脏页被成功写回到磁盘，它们就不再是脏页。InnoDB会更新重做日志的尾部位置，减少checkpoint age。

9. **日志文件管理**：随着脏页的刷新，之前记录的重做日志可以被覆盖，以便日志文件可以循环使用。

10. **性能监控与调优**：通过监控刷新操作的性能，如通过`show engine innodb status`命令，DBA可以对刷新过程进行调优，例如调整`innodb_page_cleaners`、`innodb_adaptive_flushing_lwm`、`innodb_io_capacity`等参数。

11. **刷新争用处理**：在高负载情况下，刷新操作可能会与其他数据库操作（如查询和索引构建）竞争资源，需要适当调整以避免性能瓶颈。

12. **系统状态反馈**：InnoDB通过日志和状态信息提供刷新操作的反馈，帮助DBA了解系统的当前状态并做出相应的调整。

整个刷新过程是InnoDB保证数据持久性和系统性能的关键机制之一。通过智能地管理脏页的刷新，InnoDB可以在保持高性能的同时，确保数据的安全性和完整性。

```



dirty pages

```markdown
在整个InnoDB的脏页刷新（Flush）过程中，涉及到以下特性：

1. **缓冲池（Buffer Pool）**：InnoDB使用缓冲池来缓存频繁访问的数据和索引页，减少对磁盘的I/O操作。

2. **脏页（Dirty Pages）**：在缓冲池中被修改但尚未写入磁盘的页。

3. **重做日志（Redo Log）**：事务提交前，修改操作首先记录到重做日志中，确保数据恢复的一致性。

4. **LSN（Log Sequence Number）**：用于记录重做日志的位置，每个事务都会生成一个新的LSN。

5. **Checkpoint**：刷新过程中，InnoDB会更新磁盘上数据页的状态，Checkpoint机制用于标记哪些数据已经刷新到磁盘。

6. **刷新邻接页（Flush Neighbor Page）**：当刷新一个脏页时，如果该页所在的区（extent）内还有其他脏页，它们也会被一起刷新。

7. **自适应刷新（Adaptive Flushing）**：根据当前的系统负载和性能指标，动态调整刷新速率。

8. **异步IO（Asynchronous IO）**：InnoDB支持异步IO操作，允许脏页刷新操作与其他数据库操作并发执行。

9. **刷新线程（Page Cleaner Threads）**：InnoDB使用专门的后台线程来刷新脏页。

10. **刷新控制参数**：
    - `innodb_page_cleaners`：控制刷新脏页的线程数。
    - `innodb_purge_threads`：控制清除操作的线程数。
    - `innodb_io_capacity`：定义InnoDB刷新磁盘时的IO容量。
    - `innodb_adaptive_flushing`：控制是否启用自适应刷新算法。
    - `innodb_adaptive_flushing_lwm`：自适应刷新的最低水位参数。
    - `innodb_flush_neighbors`：控制是否启用刷新邻接页特性。

11. **刷新争用处理**：在高负载情况下，InnoDB需要处理刷新操作与其他数据库操作之间的资源争用。

12. **性能监控**：通过`show engine innodb status`等命令监控刷新操作的性能。

13. **刷新风暴（Flush Storm）**：当重做日志的空间不足以容纳新的事务提交时，InnoDB会高速刷新脏页以释放空间，这可能导致性能问题。

14. **日志文件管理**：刷新过程中，InnoDB需要管理重做日志文件，确保日志文件的循环使用。

15. **系统状态反馈**：InnoDB通过日志和状态信息提供刷新操作的反馈，帮助数据库管理员了解系统的当前状态。

这些特性共同工作，确保InnoDB存储引擎能够有效地管理脏页刷新，同时保持高性能和数据的持久性。

```







